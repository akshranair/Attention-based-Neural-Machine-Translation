# -*- coding: utf-8 -*-
"""Copy of FINAL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oXTYWlMGrdrPZ2fcOD9f_4hZSHKeNBJT
"""

! pip install datasets

!pip install -U nltk

# Commented out IPython magic to ensure Python compatibility.
#import dataset
import string
import re
import random

import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

from tqdm import tqdm
from datasets import load_dataset


import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import numpy as np
# %matplotlib inline



from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')

# from normality import normalize, slugify, collapse_spaces

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")



!pip install rouge
from rouge import Rouge
import nltk
from nltk.translate.bleu_score import sentence_bleu

SOS_token = 0
EOS_token = 1

#class to convert text to numerical indices
class Lang:
    def __init__(self, name):
        self.name = name
        self.word2index = {} #a dictionary mapping words to their corresponding numerical index
        self.word2count = {}#a dictionary that maps word to its count
        self.index2word = {0: "SOS", 1: "EOS", 2: "UNK"} #defines numerical token of EOS and SOS
        self.n_words = 3  # represents total number of words in that language

    def addSentence(self, sentence):
      # add words in that sentence to the vocabulary index
        for word in sentence.split(' '):
            self.addWord(word)

    def addWord(self, word):
        if word not in self.word2index:
            self.word2index[word] = self.n_words
            self.word2count[word] = 1
            self.index2word[self.n_words] = word
            self.n_words += 1
        else:
            self.word2count[word] += 1

dataset = load_dataset("wmt16",'de-en')

#seperates training, validation and testing data
train_data = dataset['train']
val_data = dataset['validation']
test_data = dataset['test']

subset = 20000

def preprocessing(sentence,maxlen = None):
  sentence = re.sub(r"([?.!,¿])", r" \1 ", sentence)
  sentence = re.sub(r'[" "]+', " ", sentence)
  # sentence = re.sub(r"[^a-zA-Z?.!,¿]+", " ", sentence)
  sentence = sentence.strip()
  sentence = sentence.lower()
  if(maxlen != None):
    if(len(sentence.split(' ')) >=maxlen):
      return "!"
  return sentence

#storing text of english part of the dataset in a list
train_data_english = list()
train_data_german = list()
for i in tqdm(range(0,subset)):
  sent1 = preprocessing(train_data[i]['translation']['en'])
  sent2 = preprocessing(train_data[i]['translation']['de'])
  if(sent1!='!' and sent2!='!'):
    train_data_english.append(sent1)
    train_data_german.append(sent2)

#storing text of english part of the dataset in a list
val_data_english = list()
val_data_german = list()
for i in tqdm(range(0,len(val_data))):
  sent1 = preprocessing(val_data[i]['translation']['en'])
  sent2 = preprocessing(val_data[i]['translation']['de'])
  if(sent1!='!' and sent2!='!'):
    val_data_english.append(sent1)
    val_data_german.append(sent2)

test_data_english = list()
test_data_german = list()
for i in tqdm(range(0,len(test_data))):
  sent1 = preprocessing(test_data[i]['translation']['en'])
  sent2 = preprocessing(test_data[i]['translation']['de'])
  if(sent1!='!' and sent2!='!'):
    test_data_english.append(sent1)
    test_data_german.append(sent2)

print("Training data size :", len(train_data_english))
print("Validation data size :",len(val_data_english))
print("Test data size : ",len(test_data_english))

#returns the input, output language objects of class Lang and list of pair sentences
def readLangs(lang1, lang2,eng_data,german_data,reverse=False):
    pairs = []
    for i in range(0,len(german_data)):
      pair = []
      pair.append(eng_data[i])
      pair.append(german_data[i])
      pairs.append(pair)
    if reverse:
        pairs = [list(reversed(p)) for p in pairs]
        input_lang = Lang(lang2)
        output_lang = Lang(lang1)
    else:
        input_lang = Lang(lang1)
        output_lang = Lang(lang2)

    return input_lang, output_lang, pairs

def prepareData(lang1, lang2, eng_data, germ_data, reverse=False):
    input_lang, output_lang, pairs = readLangs(lang1, lang2,eng_data, germ_data, reverse)
    print("Read %s sentence pairs" % len(pairs))
    print("Counting words...")
    for pair in pairs:
        input_lang.addSentence(pair[0])
        output_lang.addSentence(pair[1])
    print("Counted words:")
    print(input_lang.name, input_lang.n_words)
    print(output_lang.name, output_lang.n_words)
    return input_lang, output_lang, pairs


input_lang, output_lang, pairs = prepareData('en', 'de',train_data_english, train_data_german,False)
_,_,val_pairs = readLangs('en', 'de',val_data_english, val_data_german,False)
print(random.choice(pairs))

_,_,test_pairs = readLangs('en', 'de',test_data_english, test_data_german,False)

def indexesFromSentence(lang, sentence):
  # x = [lang.word2index[word] for word in sentence.split(' ')]
  x = []
  for word in sentence.split(' '):
    try:
      index = lang.word2index[word]
    except:
      index = 2
    x.append(index)

  # padding = 50 - len(x)
  # x.extend([0] * padding)
  return x

# returns pytorch tensors of tokens in a sentence and appends EOS
def tensorFromSentence(lang, sentence):
    indexes = indexesFromSentence(lang, sentence)
    indexes.append(EOS_token)
    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)


 #returns pair of tensors of sentence
def tensorsFromPair(pair):
    input_tensor = tensorFromSentence(input_lang, pair[0])
    target_tensor = tensorFromSentence(output_lang, pair[1])
    return (input_tensor, target_tensor)

subset2 = 5000

#creating tensors of english and german training data
pair_tensors = list()
english_tensors = []
german_tensors = []
maxlen = 0
for i in pairs[:subset2]:
  tp = tensorsFromPair(i)
  pair_tensors.append(tp)
  maxlen = max(maxlen,len(tp[0]))
  english_tensors.append(tp[0])
  german_tensors.append(tp[1])

maxlen

#creating validation of english and german data
val_pair_tensors = list()
val_english_tensors = []
val_german_tensors = []
for i in val_pairs:
  tp = tensorsFromPair(i)
  val_pair_tensors.append(tp)
  val_english_tensors.append(tp[0])
  val_german_tensors.append(tp[1])

#creating testing of english and german data
test_pair_tensors = list()
test_english_tensors = []
test_german_tensors = []
for i in test_pairs:
  tp = tensorsFromPair(i)
  test_pair_tensors.append(tp)
  test_english_tensors.append(tp[0])
  test_german_tensors.append(tp[1])

Maxlen = 4*maxlen

"""## HYPERPARAMETERS"""

EPOCHS = 10
SUBSET = 5000
learning_rate = 0.01
hidden_size = 256



"""### MODEL A : NON- CONTEXTUAL EMBEDDINGS + LSTM



"""

'''The EncoderLSTM class  encodes the input sequence into hidden states using an LSTM layer. 
      It has an embedding layer to convert input tokens into continuous vector representations, an LSTM layer 
      to process the embedded input, and methods to initialize the hidden state and cell state. The forward method
       implements the forward pass of the encoder, taking input tensor, hidden state, and cell state as inputs,
        and returning the output tensor, updated hidden state, and cell state.
'''
class EncoderLSTM(nn.Module):
  #encode the input sequence
    def __init__(self, input_size, hidden_size, num_layers=1):
      #constructor of the parent class to initialize EncoderLSTM
        super(EncoderLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        self.embedding = nn.Embedding(input_size, hidden_size)
        #makes the embedding layer trainable
        self.embedding.weight.requires_grad = True
        #lstm layer
        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers)

    def forward(self, input, hidden, cell): 
        embedded = self.embedding(input).view(1, 1, -1) #lstm expects 3D tensor with shape of (sequence_length, batch_size, input_size)
        output, (hidden, cell) = self.lstm(embedded, (hidden, cell)) #passes embeddings along with previous hidden and cell state
        return output, hidden, cell

    def initHidden(self):
        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device) 

    def initCell(self):
        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)


'''DecoderLSTM class  decodes hidden states into an output sequence using an LSTM layer.
It has an embedding layer to convert output tokens into continuous vector representations, an LSTM layer 
to process the embedded output, and methods to initialize the hidden state and cell state. The forward method 
implements the forward pass of the decoder, taking input tensor, hidden state, and cell state as inputs, and 
returning the output tensor, updated hidden state, and cell state. The output is passed through a softmax activation
 function to obtain a probability distribution over the output vocabulary, and the log-softmax is used for numerical stability.

'''

class DecoderLSTM(nn.Module):
    def __init__(self, hidden_size, output_size, num_layers=1):
        super(DecoderLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.num_layers = num_layers

        self.embedding = nn.Embedding(output_size, hidden_size)
        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers)
        self.out = nn.Linear(hidden_size, output_size)
        self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, input, hidden, cell):
        output = self.embedding(input).view(1, 1, -1)
        output = F.relu(output)
        output, (hidden, cell) = self.lstm(output, (hidden, cell))
        output = self.softmax(self.out(output[0]))
        return output, hidden, cell

    def initHidden(self):
        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)

    def initCell(self):
        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)



'''The train function implements the training loop for a sequence-to-sequence model with encoder-decoder architecture.
 It initializes the encoder and decoder hidden states and cell states, resets the optimizer gradients, iterates through 
 input sequence tokens using the encoder and saves encoder outputs, initializes the decoder with the SOS token, iterates 
 through target sequence tokens using the decoder, computes loss at each time step, accumulates loss, backpropagates, and
  updates optimizer parameters. It returns the average loss per target token.'''


def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=Maxlen):
    encoder_hidden = encoder.initHidden() #initialize hidden state with 0`
    encoder_cell = encoder.initCell() #initialize cell state with 0

    encoder_optimizer.zero_grad() #reset the gradients of encoder_optimizer
    decoder_optimizer.zero_grad() #reset the gradients of decoder_optimizer

    input_length = input_tensor.size(0) 
    target_length = target_tensor.size(0)
    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

    loss = 0

    #encoder output for each token is saved in encoder_outputs
    for ei in range(input_length):
        encoder_output, encoder_hidden, encoder_cell = encoder(input_tensor[ei], encoder_hidden, encoder_cell)
        encoder_outputs[ei] = encoder_output[0, 0] #lstm outputs hidden state for each time step, but we are interested in the last time step

    decoder_input = torch.tensor([[SOS_token]], device=device) # keeping first token as SOS
    decoder_hidden = encoder_hidden
    decoder_cell = encoder_cell

    for di in range(target_length):
        decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)
        topv, topi = decoder_output.topk(1)  #take the top 1 predicted index
        decoder_input = topi.squeeze().detach() #remove the extra dimensions

        loss += criterion(decoder_output, target_tensor[di])

        #loop continues until EOS token is encountered
        if decoder_input.item() == EOS_token:
            break

    #backpropagated
    loss.backward()

    encoder_optimizer.step()
    decoder_optimizer.step()

    return loss.item() / target_length

'''evaluates the model during inference or validation, and calculates the average loss per target token for a given input sequence
 and target sequence. It also initializes the encoder and decoder hidden states and cell states, resets the optimizer gradients, 
 iterates through input sequence tokens using the encoder and saves encoder outputs, initializes the decoder with the SOS token, 
 iterates through target sequence tokens using the decoder, computes loss at each time step, accumulates loss, and returns the
  average loss per target token.'''

def eval(input_tensor, target_tensor, encoder, decoder, criterion, max_length=Maxlen):
    encoder_hidden = encoder.initHidden()
    encoder_cell = encoder.initCell()

    input_length = input_tensor.size(0)
    target_length = target_tensor.size(0)

    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

    loss = 0

    with torch.no_grad():  # Disable gradient computation for inference
        for ei in range(input_length):
            encoder_output, encoder_hidden, encoder_cell = encoder(input_tensor[ei], encoder_hidden, encoder_cell)
            encoder_outputs[ei] = encoder_output[0, 0]

        decoder_input = torch.tensor([[SOS_token]], device=device)

        decoder_hidden = encoder_hidden
        decoder_cell = encoder_cell

        for di in range(target_length):
            decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)
            topv, topi = decoder_output.topk(1)
            decoder_input = topi.squeeze().detach()

            loss += criterion(decoder_output, target_tensor[di])

            if decoder_input.item() == EOS_token:
                break

    return loss.item() / target_length

def eval(input_tensor, target_tensor, encoder, decoder, criterion, max_length=Maxlen):
    encoder_hidden = encoder.initHidden()
    encoder_cell = encoder.initCell()


    input_length = input_tensor.size(0)
    target_length = target_tensor.size(0)

    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

    loss = 0

    with torch.no_grad():  # Disable gradient computation for inference
        for ei in range(input_length):
            encoder_output, encoder_hidden, encoder_cell = encoder(input_tensor[ei], encoder_hidden, encoder_cell)
            encoder_outputs[ei] = encoder_output[0, 0]

        decoder_input = torch.tensor([[SOS_token]], device=device)

        decoder_hidden = encoder_hidden
        decoder_cell = encoder_cell

        for di in range(target_length):
            decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)
            topv, topi = decoder_output.topk(1)
            decoder_input = topi.squeeze().detach()

            loss += criterion(decoder_output, target_tensor[di])

            if decoder_input.item() == EOS_token:
                break

    return loss.item() / target_length

def eval(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=Maxlen):
    encoder_hidden = encoder.initHidden()
    encoder_cell = encoder.initCell()

    input_length = input_tensor.size(0)
    target_length = target_tensor.size(0)

    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

    loss = 0

    with torch.no_grad():  # Disable gradient computation for inference
        for ei in range(input_length):
            encoder_output, encoder_hidden, encoder_cell = encoder(input_tensor[ei], encoder_hidden, encoder_cell)
            encoder_outputs[ei] = encoder_output[0, 0]

        decoder_input = torch.tensor([[SOS_token]], device=device)

        decoder_hidden = encoder_hidden
        decoder_cell = encoder_cell

        for di in range(target_length):
            decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)
            topv, topi = decoder_output.topk(1)
            decoder_input = topi.squeeze().detach()

            loss += criterion(decoder_output, target_tensor[di])

            if decoder_input.item() == EOS_token:
                break

    return loss.item() / target_length



'''. It takes an input sentence, passes it through the encoder,
 generates output words using the decoder, and returns a list of decoded words.'''
def evaluate(encoder, decoder, sentence, max_length=Maxlen):
    with torch.no_grad():
        input_tensor = sentence
        input_length = input_tensor.size()[0]
        encoder_hidden = encoder.initHidden()
        encoder_cell = encoder.initCell()

        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

        for ei in range(input_length):
            encoder_output, encoder_hidden, encoder_cell = encoder(input_tensor[ei], encoder_hidden, encoder_cell)
            encoder_outputs[ei] += encoder_output[0, 0]

        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS

        decoder_hidden = encoder_hidden
        decoder_cell = encoder_cell

        decoded_words = []
        for di in range(max_length):
            decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)
            topv, topi = decoder_output.data.topk(1)
            if topi.item() == EOS_token:
                decoded_words.append('<EOS>')
                break
            else:
                decoded_words.append(output_lang.index2word[topi.item()])

            decoder_input = topi.squeeze().detach()

        return decoded_words

encoder = EncoderLSTM(input_lang.n_words, hidden_size).to(device)
decoder = DecoderLSTM(hidden_size, output_lang.n_words).to(device)







encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)
decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)

training_pairs = pair_tensors
plot_losses = []
# print_loss=int(len(training_pairs)*0.5)
print_loss_total = 0  # Reset every print_every
plot_loss_total = 0 
epochs = 10
plot_train_losses = []
plot_val_losses=[]

for ep in range(epochs):
  encoder.train()
  decoder.train()
  t_loss = 0
  v_loss = 0
  for i in tqdm(range(int(len(training_pairs)))):
    training_pair = training_pairs[i]
    input_tensor = training_pair[0]
    target_tensor = training_pair[1]
    criterion = nn.NLLLoss()
    train_loss = train(input_tensor, target_tensor, encoder,decoder, encoder_optimizer, decoder_optimizer, criterion)
    t_loss+=train_loss
  
  encoder.eval()
  decoder.eval()
  for i in tqdm(range(int(len(val_pair_tensors)))):
    training_pair = val_pair_tensors[i]
    input_tensor = training_pair[0]
    target_tensor = training_pair[1]
    criterion = nn.NLLLoss()
    val_loss = eval(input_tensor, target_tensor, encoder,decoder, encoder_optimizer, decoder_optimizer,criterion)
    v_loss+=val_loss
  
  plot_train_losses.append(train_loss)
  plot_val_losses.append(val_loss)



x = np.arange(1,epochs+1)
plt.figure(figsize=(20,10))
plt.plot(x,plot_train_losses,'--bo',linewidth=2,label ='train',)
plt.plot(x,plot_val_losses,'--ro',linewidth=2,label = 'val')
plt.title("Validation and training Losses vs Epochs ")
plt.xticks(x)
plt.xlabel("Number of epochs")
plt.ylabel("Losses")
plt.legend()
plt.show()





test_data_prediction = list()
test_data_original = list()
for i,sent in tqdm(enumerate(test_english_tensors)):
    output_words = evaluate(encoder, decoder, sent)
    output_sentence = ' '.join(output_words)
    test_data_prediction.append(output_sentence)
    test_data_original.append(test_data_german[i])

len(test_pair_tensors)

# create lists to hold individual scores for each sentence
bleu_1_scores = []
bleu_2_scores = []
rouge_l_scores = []

# initialize rouge and bleu scorers
rouge = Rouge()
bleu = nltk.translate.bleu_score.SmoothingFunction()

# iterate over each sentence in the reference and generated lists
for ref_sent, gen_sent in zip(test_data_original, test_data_prediction):
    # compute bleu scores for n-grams of length 1 and 2
    bleu_1 = sentence_bleu([ref_sent.split()], gen_sent.split(), weights=(1, 0, 0), smoothing_function=bleu.method1)
    bleu_2 = sentence_bleu([ref_sent.split()], gen_sent.split(), weights=(0.5, 0.5, 0), smoothing_function=bleu.method1)
    
    # compute rouge-l score
    scores = rouge.get_scores(gen_sent, ref_sent)[0]
    rouge_l = scores['rouge-l']['f']
    
    # add individual scores to lists
    bleu_1_scores.append(bleu_1)
    bleu_2_scores.append(bleu_2)
    rouge_l_scores.append(rouge_l)

# compute overall scores as average of individual scores
avg_bleu_1 = sum(bleu_1_scores) / len(bleu_1_scores)
avg_bleu_2 = sum(bleu_2_scores) / len(bleu_2_scores)
avg_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)

print(f'Bleu-1 score: {avg_bleu_1:.4f}')
print(f'Bleu-2 score: {avg_bleu_2:.4f}')
print(f'Rouge-L score: {avg_rouge_l:.4f}')

"""### MODEL B WITH NON - CONTEXTUAL EMBEDDINGS + LSTM + GLOBAL ATTENTION

---


"""

#SAME AS BEFORE

class EncoderLSTM(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(EncoderLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(input_size, hidden_size)
        self.embedding.weight.requires_grad = True
        self.lstm = nn.LSTM(hidden_size, hidden_size)

    def forward(self, input, hidden, cell):
        embedded = self.embedding(input).view(1, 1, -1)
        output = embedded
        output, (hidden, cell) = self.lstm(output, (hidden, cell))
        return output, hidden, cell

    def initHidden(self):
        return torch.zeros(1, 1, self.hidden_size, device=device)

    def initCell(self):
        return torch.zeros(1, 1, self.hidden_size, device=device)

'''The AttnDecoderLSTM class implements a LSTM-based decoder model with attention mechanism. 
It takes an input token, hidden state, cell state, and encoder outputs as input, and produces an output prediction,
 updated hidden state, cell state, and attention weights as output. It includes an embedding layer for input token embeddings,
  computes attention weights using a linear layer, combines input token embeddings and attention-applied encoder outputs using
   another linear layer, and passes the concatenated result through an LSTM layer. The final output is obtained through a linear
    layer with a log-softmax activation function. The initHidden() and initCell() methods initialize the hidden state and cell state
     with zeros, respectively.'''


class AttnDecoderLSTM(nn.Module):
    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=Maxlen):
        super(AttnDecoderLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.dropout_p = dropout_p
        self.max_length = max_length

        self.embedding = nn.Embedding(self.output_size, self.hidden_size)
        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)
        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)
        self.dropout = nn.Dropout(self.dropout_p)
        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)
        self.out = nn.Linear(self.hidden_size, self.output_size)

    def forward(self, input, hidden, cell, encoder_outputs):
        embedded = self.embedding(input).view(1, 1, -1)
        embedded = self.dropout(embedded)

        attn_weights = F.softmax(
            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)
        attn_applied = torch.bmm(attn_weights.unsqueeze(0),
                                 encoder_outputs.unsqueeze(0))

        output = torch.cat((embedded[0], attn_applied[0]), 1)
        output = self.attn_combine(output).unsqueeze(0)

        output = F.relu(output)
        output, (hidden, cell) = self.lstm(output, (hidden, cell))

        output = F.log_softmax(self.out(output[0]), dim=1)
        return output, hidden, cell, attn_weights

    def initHidden(self):
        return torch.zeros(1, 1, self.hidden_size, device=device)

    def initCell(self):
        return torch.zeros(1, 1, self.hidden_size, device=device)

teacher_forcing_ratio = 0.5
'''Teacher forcing is used during training in seq2seq models where the target sequence is used 
as input to the decoder at each time step, instead of using the decoder's own predictions. This can help in 
stabilizing the training process, but may result in suboptimal performance during inference when the model is 
generating sequences independently. The teacher_forcing_ratio determines the probability of using teacher forcing
 during training. In this case, it is set to 0.5, which means that there is a 50% chance of using teacher forcing 
 at each time step during training.'''

def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=Maxlen):
    encoder_hidden = encoder.initHidden()
    encoder_cell = encoder.initCell()

    encoder_optimizer.zero_grad()
    decoder_optimizer.zero_grad()

    input_length = input_tensor.size(0)
    target_length = target_tensor.size(0)

    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

    loss = 0

    for ei in range(input_length):
        encoder_output, encoder_hidden, encoder_cell = encoder(
            input_tensor[ei], encoder_hidden, encoder_cell)
        encoder_outputs[ei] = encoder_output[0, 0]

    decoder_input = torch.tensor([[SOS_token]], device=device)

    decoder_hidden = encoder_hidden
    decoder_cell = encoder_cell

    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False




    if use_teacher_forcing:
        # Teacher forcing: Feed the target as the next input
        for di in range(target_length):
            decoder_output, decoder_hidden, decoder_cell, decoder_attention = decoder(
                decoder_input, decoder_hidden, decoder_cell, encoder_outputs)
            loss += criterion(decoder_output, target_tensor[di])
            decoder_input = target_tensor[di].view(1, -1)  # Teacher forcing

    else:
        # Without teacher forcing: use its own predictions as the next input
        for di in range(target_length):
            decoder_output, decoder_hidden, decoder_cell, decoder_attention = decoder(
                decoder_input, decoder_hidden, decoder_cell, encoder_outputs)
            topv, topi = decoder_output.topk(1)
            decoder_input = topi.squeeze().detach().view(1, -1)  # detach from history as input

            loss += criterion(decoder_output, target_tensor[di])
            if decoder_input.item() == EOS_token:
                break

    loss.backward()

    encoder_optimizer.step()
    decoder_optimizer.step()

    return loss.item() / target_length

def eval(input_tensor, target_tensor, encoder, decoder, criterion, max_length=Maxlen):
    encoder_hidden = encoder.initHidden()
    encoder_cell = encoder.initCell()

    input_length = input_tensor.size(0)
    target_length = target_tensor.size(0)

    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

    loss = 0

    for ei in range(input_length):
        encoder_output, encoder_hidden, encoder_cell = encoder(
            input_tensor[ei], encoder_hidden, encoder_cell)
        encoder_outputs[ei] = encoder_output[0, 0]

    decoder_input = torch.tensor([[SOS_token]], device=device)

    decoder_hidden = encoder_hidden
    decoder_cell = encoder_cell

    for di in range(target_length):
        decoder_output, decoder_hidden, decoder_cell, decoder_attention = decoder(
            decoder_input, decoder_hidden, decoder_cell, encoder_outputs)
        topv, topi = decoder_output.topk(1)
        decoder_input = topi.squeeze().detach().view(1, -1)

        loss += criterion(decoder_output, target_tensor[di])

    return loss.item() / target_length

hidden_size = 256
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
encoder = EncoderLSTM(input_lang.n_words, hidden_size).to(device)
decoder = AttnDecoderLSTM(hidden_size, output_lang.n_words).to(device)
learning_rate = 0.01
encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)
decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)

training_pairs = pair_tensors
plot_losses = []
print_loss=int(len(training_pairs)*0.5)
print_loss_total = 0  # Reset every print_every
plot_loss_total = 0 
plot_train_losses = []
plot_val_losses=[]
for ep in range(epochs):
  encoder.train()
  decoder.train()
  for i in tqdm(range(int(len(training_pairs)))):
    training_pair = training_pairs[i]
    input_tensor = training_pair[0]
    target_tensor = training_pair[1]
    criterion = nn.NLLLoss()
    train_loss = train(input_tensor, target_tensor, encoder,decoder, encoder_optimizer, decoder_optimizer, criterion)

  
  encoder.eval()
  decoder.eval()
  for i in tqdm(range(int(len(val_pair_tensors)))):
    training_pair = val_pair_tensors[i]
    input_tensor = training_pair[0]
    target_tensor = training_pair[1]
    criterion = nn.NLLLoss()
    val_loss = eval(input_tensor, target_tensor, encoder,decoder, criterion)
  
  plot_train_losses.append(train_loss)
  plot_val_losses.append(val_loss)

x = np.arange(1,epochs+1)
plt.figure(figsize=(20,10))
plt.plot(x,plot_train_losses,'--bo',linewidth=2,label ='train',)
plt.plot(x,plot_val_losses,'--ro',linewidth=2,label = 'val')
plt.title("Validation and training Losses vs Epochs ")
plt.xticks(x)
plt.xlabel("Number of epochs")
plt.ylabel("Losses")
plt.legend()
plt.show()



plot_train_losses

plot_val_losses

def evaluate(encoder, decoder, sentence, max_length=Maxlen):
    with torch.no_grad():
        input_tensor = sentence
        input_length = input_tensor.size()[0]
        encoder_hidden = encoder.initHidden()
        encoder_cell = encoder.initCell()

        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

        for ei in range(input_length):
            encoder_output, encoder_hidden, encoder_cell = encoder(input_tensor[ei], encoder_hidden, encoder_cell)
            encoder_outputs[ei] += encoder_output[0, 0]

        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS

        decoder_hidden = encoder_hidden
        decoder_cell = encoder_cell

        decoded_words = []
        for di in range(max_length):
            decoder_output, decoder_hidden, decoder_cell, decoder_attention = decoder(decoder_input, decoder_hidden, decoder_cell,encoder_outputs)
            topv, topi = decoder_output.data.topk(1)
            if topi.item() == EOS_token:
                decoded_words.append('<EOS>')
                break
            else:
                decoded_words.append(output_lang.index2word[topi.item()])

            decoder_input = topi.squeeze().detach()

        return decoded_words



test_data_prediction = list()
test_data_original = list()
for i,sent in tqdm(enumerate(test_english_tensors)):
    output_words = evaluate(encoder, decoder, sent)
    output_sentence = ' '.join(output_words)
    test_data_prediction.append(output_sentence)
    test_data_original.append(test_data_german[i])

# test_data_prediction

!pip install rouge
from rouge import Rouge
import nltk
from nltk.translate.bleu_score import sentence_bleu
# from nltk.translate.rouge import Rouge

# assuming you have two lists of sentences: `reference` and `generated`
# where each element in both lists is a sentence string

# create lists to hold individual scores for each sentence
bleu_1_scores = []
bleu_2_scores = []
rouge_l_scores = []

# initialize rouge and bleu scorers
rouge = Rouge()
bleu = nltk.translate.bleu_score.SmoothingFunction()

# iterate over each sentence in the reference and generated lists
for ref_sent, gen_sent in zip(test_data_original, test_data_prediction):
    # compute bleu scores for n-grams of length 1 and 2
    bleu_1 = sentence_bleu([ref_sent.split()], gen_sent.split(), weights=(1, 0, 0), smoothing_function=bleu.method1)
    bleu_2 = sentence_bleu([ref_sent.split()], gen_sent.split(), weights=(0.5, 0.5, 0), smoothing_function=bleu.method1)
    
    # compute rouge-l score
    scores = rouge.get_scores(gen_sent, ref_sent)[0]
    rouge_l = scores['rouge-l']['f']
    
    # add individual scores to lists
    bleu_1_scores.append(bleu_1)
    bleu_2_scores.append(bleu_2)
    rouge_l_scores.append(rouge_l)

# compute overall scores as average of individual scores
avg_bleu_1 = sum(bleu_1_scores) / len(bleu_1_scores)
avg_bleu_2 = sum(bleu_2_scores) / len(bleu_2_scores)
avg_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)

print(f'Bleu-1 score: {avg_bleu_1:.4f}')
print(f'Bleu-2 score: {avg_bleu_2:.4f}')
print(f'Rouge-L score: {avg_rouge_l:.4f}')

!pip install rouge
from rouge import Rouge
import nltk
from nltk.translate.bleu_score import sentence_bleu
# from nltk.translate.rouge import Rouge

# assuming you have two lists of sentences: `reference` and `generated`
# where each element in both lists is a sentence string

# create lists to hold individual scores for each sentence
bleu_1_scores = []
bleu_2_scores = []
rouge_l_scores = []

# initialize rouge and bleu scorers
rouge = Rouge()
bleu = nltk.translate.bleu_score.SmoothingFunction()

# iterate over each sentence in the reference and generated lists
for ref_sent, gen_sent in zip(test_data_original, test_data_prediction):
    # compute bleu scores for n-grams of length 1 and 2
    bleu_1 = sentence_bleu([ref_sent.split()], gen_sent.split(), weights=(1, 0, 0), smoothing_function=bleu.method1)
    bleu_2 = sentence_bleu([ref_sent.split()], gen_sent.split(), weights=(0.5, 0.5, 0), smoothing_function=bleu.method1)
    
    # compute rouge-l score
    scores = rouge.get_scores(gen_sent, ref_sent)[0]
    rouge_l = scores['rouge-l']['f']
    
    # add individual scores to lists
    bleu_1_scores.append(bleu_1)
    bleu_2_scores.append(bleu_2)
    rouge_l_scores.append(rouge_l)

# compute overall scores as average of individual scores
avg_bleu_1 = sum(bleu_1_scores) / len(bleu_1_scores)
avg_bleu_2 = sum(bleu_2_scores) / len(bleu_2_scores)
avg_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)


print(f'Bleu-1 score: {avg_bleu_1:.4f}')
print(f'Bleu-2 score: {avg_bleu_2:.4f}')
print(f'Rouge-L score: {avg_rouge_l:.4f}')



"""MODEL C: NON CONTEXTUALIZED EMBEDDINGS + LOCAL ATTENTION"""



'''It computes attention weights for a subset of encoder outputs within a local window centered around the current input token(s) in the decoder. 
The attention weights are computed using a linear layer and softmax activation. The context vector is computed as a weighted sum of the encoder 
outputs within the local window. The context vector is combined with the input using another linear layer and ReLU activation, and then passed 
through an LSTM layer to compute the next hidden state and output. This local attention mechanism allows the decoder to focus on a subset of encoder 
outputs within a local window during the decoding process.'''

class AttnDecoderLSTM(nn.Module):
    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=100, window_size=5):
        super(AttnDecoderLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.dropout_p = dropout_p
        self.max_length = max_length
        self.window_size = window_size  # Added window_size as a parameter

        self.embedding = nn.Embedding(self.output_size, self.hidden_size)
        # a linear layer that computes the attention weights based on concatenation of the embedded input and the hidden state of the LSTM
        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)
        # A linear layer that combines the attention weights and embedded input
        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)
        self.dropout = nn.Dropout(self.dropout_p)
        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)
        self.out = nn.Linear(self.hidden_size, self.output_size)

    def forward(self, input, hidden, cell, encoder_outputs):
        embedded = self.embedding(input).view(1, 1, -1)
        embedded = self.dropout(embedded)

        # Determine the starting and ending indices of the local attention window
        current_position = input.size(0)  # Assuming input is a single token
        start_idx = max(0, current_position - self.window_size // 2) # calculates the starting and ending index to centre it around current input token
        end_idx = min(encoder_outputs.size(0), start_idx + self.window_size)

        # Extract the subset of encoder outputs within the local attention window
        subset_encoder_outputs = encoder_outputs[start_idx:end_idx, :]

        # Compute attention weights for the subset of encoder outputs
        attn_weights = F.softmax(
            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)
        attn_weights = attn_weights[:, start_idx:end_idx]  # Extract the attention weights for the subset

        # Use the computed attention weights to compute the context vector
        attn_applied = torch.bmm(attn_weights.unsqueeze(0),
                                 subset_encoder_outputs.unsqueeze(0))

        output = torch.cat((embedded[0], attn_applied[0]), 1)
        output = self.attn_combine(output).unsqueeze(0)

        output = F.relu(output)
        output, (hidden, cell) = self.lstm(output, (hidden, cell))

        output = F.log_softmax(self.out(output[0]), dim=1)
        return output, hidden, cell, attn_weights



encoder = EncoderLSTM(input_lang.n_words, hidden_size).to(device)
decoder = AttnDecoderLSTM(hidden_size, output_lang.n_words).to(device)
learning_rate = 0.01
encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)
decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)

training_pairs = pair_tensors
plot_losses = []
print_loss_total = 0  # Reset every print_every
plot_loss_total = 0 
epochs = 10
plot_train_losses = []
plot_val_losses=[]
for ep in range(epochs):
  encoder.train()
  decoder.train()
  for i in tqdm(range(int(len(training_pairs)))):
    training_pair = training_pairs[i]
    input_tensor = training_pair[0]
    target_tensor = training_pair[1]
    criterion = nn.NLLLoss()
    train_loss = train(input_tensor, target_tensor, encoder,decoder, encoder_optimizer, decoder_optimizer, criterion)

  
  encoder.eval()
  decoder.eval()
  for i in tqdm(range(int(len(val_pair_tensors)))):
    training_pair = val_pair_tensors[i]
    input_tensor = training_pair[0]
    target_tensor = training_pair[1]
    criterion = nn.NLLLoss()
    val_loss = eval(input_tensor, target_tensor, encoder,decoder, criterion)
  
  plot_train_losses.append(train_loss)
  plot_val_losses.append(val_loss)

x = np.arange(1,epochs+1)
plt.figure(figsize=(20,10))
plt.plot(x,plot_train_losses,'--bo',linewidth=2,label ='train',)
plt.plot(x,plot_val_losses,'--ro',linewidth=2,label = 'val')
plt.title("Validation and training Losses vs Epochs ")
plt.xticks(x)
plt.xlabel("Number of epochs")
plt.ylabel("Losses")
plt.legend()
plt.show()







def evaluate(encoder, decoder, sentence, max_length=Maxlen):
    with torch.no_grad():
        input_tensor = sentence
        input_length = input_tensor.size()[0]
        encoder_hidden = encoder.initHidden()
        encoder_cell = encoder.initCell()

        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

        for ei in range(input_length):
            encoder_output, encoder_hidden, encoder_cell = encoder(input_tensor[ei], encoder_hidden, encoder_cell)
            encoder_outputs[ei] += encoder_output[0, 0]

        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS

        decoder_hidden = encoder_hidden
        decoder_cell = encoder_cell

        decoded_words = []
        for di in range(max_length):
            decoder_output, decoder_hidden, decoder_cell, decoder_attention = decoder(decoder_input, decoder_hidden, decoder_cell,encoder_outputs)
            topv, topi = decoder_output.data.topk(1)
            if topi.item() == EOS_token:
                decoded_words.append('<EOS>')
                break
            else:
                decoded_words.append(output_lang.index2word[topi.item()])


        return decoded_words



test_data_prediction = list()
test_data_original = list()
for i,sent in tqdm(enumerate(test_english_tensors)):
    output_words = evaluate(encoder, decoder, sent)
    output_sentence = ' '.join(output_words)
    test_data_prediction.append(output_sentence)
    test_data_original.append(test_data_german[i])



bleu_1_scores = []
bleu_2_scores = []
rouge_l_scores = []

# initialize rouge and bleu scorers
rouge = Rouge()
bleu = nltk.translate.bleu_score.SmoothingFunction()

# iterate over each sentence in the reference and generated lists
for ref_sent, gen_sent in zip(test_data_original, test_data_prediction):
    # compute bleu scores for n-grams of length 1 and 2
    bleu_1 = sentence_bleu([ref_sent.split()], gen_sent.split(), weights=(1, 0, 0), smoothing_function=bleu.method1)
    bleu_2 = sentence_bleu([ref_sent.split()], gen_sent.split(), weights=(0.5, 0.5, 0), smoothing_function=bleu.method1)
    
    # compute rouge-l score
    scores = rouge.get_scores(gen_sent, ref_sent)[0]
    rouge_l = scores['rouge-l']['f']
    
    # add individual scores to lists
    bleu_1_scores.append(bleu_1)
    bleu_2_scores.append(bleu_2)
    rouge_l_scores.append(rouge_l)

# compute overall scores as average of individual scores
avg_bleu_1 = sum(bleu_1_scores) / len(bleu_1_scores)
avg_bleu_2 = sum(bleu_2_scores) / len(bleu_2_scores)
avg_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)

print(f'Bleu-1 score: {avg_bleu_1:.4f}')
print(f'Bleu-2 score: {avg_bleu_2:.4f}')
print(f'Rouge-L score: {avg_rouge_l:.4f}')

